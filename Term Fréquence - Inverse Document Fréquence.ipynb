{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus - Collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docA = \"Sudya is comming from Lyon\"\n",
    "docB = \"Tejas is comming from Paris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time while working on Text we use BOW (Bag of words) model to represent the document, Each document can be thought as BOW  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sudya', 'is', 'comming', 'from', 'Lyon']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the document into componant words called <b>'Tokenizing\"</b>\n",
    "so documents are tokenized then,\n",
    "\n",
    "we need to convert Tokenized BOW to numbers for furether process!\n",
    "\n",
    "Way 1 - Create vector of all possible words and for each document count how many times each word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lyon', 'Paris', 'Sudya', 'Tejas', 'comming', 'from', 'is'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSet = set(bowA).union(set(bowB)) # create unique set of words from bag\n",
    "wordSet\n",
    "#type(wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries. to keep word counts\n",
    "\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "#wordDictA\n",
    "#type(wordDictA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will count words in our bag \n",
    "\n",
    "for word in bowA:\n",
    "    wordDictA[word]=1\n",
    "    \n",
    "for word in bowB:\n",
    "    wordDictB[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lyon': 1,\n",
       " 'Paris': 0,\n",
       " 'Sudya': 1,\n",
       " 'Tejas': 0,\n",
       " 'comming': 1,\n",
       " 'from': 1,\n",
       " 'is': 1}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lyon': 0,\n",
       " 'Paris': 1,\n",
       " 'Sudya': 0,\n",
       " 'Tejas': 1,\n",
       " 'comming': 1,\n",
       " 'from': 1,\n",
       " 'is': 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyon</th>\n",
       "      <th>Paris</th>\n",
       "      <th>Sudya</th>\n",
       "      <th>Tejas</th>\n",
       "      <th>comming</th>\n",
       "      <th>from</th>\n",
       "      <th>is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lyon  Paris  Sudya  Tejas  comming  from  is\n",
       "0     1      0      1      0        1     1   1\n",
       "1     0      1      0      1        1     1   1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we think that the words which appear more often should have a greater weight in textual data analysis,\n",
    "\n",
    "but that’s not always the case. Words such as “the”, “can”, and “He/She” — called <b>stopwords</b> — appear the most in a corpus of text, \n",
    "but are of very little significance. \n",
    "\n",
    "Instead, the words which are rare are the ones that actually help in distinguishing between the data, and carry more weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t — term (word)\n",
    "\n",
    "d — document (set of words)\n",
    "\n",
    "N — count of corpus\n",
    "\n",
    "<b>Term Frequency (TF) </b> : tf(t,d) = count of t in d / number of words in d\n",
    "\n",
    "\n",
    "Now for IDF, there are few other problems. \n",
    "\n",
    "- In case of a large corpus, say 20,000, the IDF value explodes. So to dampen the effect we take log of IDF.\n",
    "\n",
    "- During the query time, when a word which is not in vocab occurs, the df will be 0. As we cannot divide by 0, we smoothen the     value by adding 1 to the denominator.\n",
    "\n",
    "<b>Inverse Document Frequency (IDF)</b>  : idf(t) = log(N/(df + 1))\n",
    "\n",
    "hence \n",
    "\n",
    "<b> tf-idf(t, d) = tf(t, d) * log(N/(df + 1))</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will compute term TF as below\n",
    "\n",
    "def term_Frequency(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bowCount)\n",
    "    return tfDict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_bowA = term_Frequency(wordDictA, bowA)\n",
    "tf_bowB = term_Frequency(wordDictB, bowB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will compute term IDF as below\n",
    "def Inverse_term_frequency(docList):\n",
    "    import math\n",
    "    idf_dict = {}\n",
    "    n = len(docList) # length of no of documents\n",
    "    \n",
    "    # count the no of documnts that contain word 'X'\n",
    "    idf_dict = dict.fromkeys(docList[0].keys(), 0)       \n",
    "    for doc in docList:  # iterate over evey doc\n",
    "        for word , val in doc.items():  # iterate over evey word\n",
    "            if val > 0:\n",
    "                idf_dict[word] = idf_dict[word] + 1 # words that exist , that we increment by 1\n",
    "                \n",
    "    # divide N by denominator and take the log of that\n",
    "    for word , val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(n / float(val))\n",
    "        \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_all = Inverse_term_frequency([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF - IDF \n",
    "def compute_TF_IDF(tf_Bow, idf_all):\n",
    "    tfidf = {}\n",
    "    for word, val in tf_Bow.items():\n",
    "        tfidf[word] = val * idf_all[word]\n",
    "    return tfidf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBowA = compute_TF_IDF(tf_bowA, idf_all)\n",
    "tfidfBowB = compute_TF_IDF(tf_bowB, idf_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyon</th>\n",
       "      <th>Paris</th>\n",
       "      <th>Sudya</th>\n",
       "      <th>Tejas</th>\n",
       "      <th>comming</th>\n",
       "      <th>from</th>\n",
       "      <th>is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lyon     Paris     Sudya     Tejas  comming  from   is\n",
       "0  0.138629  0.000000  0.138629  0.000000      0.0   0.0  0.0\n",
       "1  0.000000  0.138629  0.000000  0.138629      0.0   0.0  0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
